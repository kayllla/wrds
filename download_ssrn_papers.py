#!/usr/bin/env python3
"""
SSRN è®ºæ–‡æ‰¹é‡ä¸‹è½½è„šæœ¬
ä» ruotong.json è¯»å– URL åˆ—è¡¨ï¼Œä¸‹è½½æ‰€æœ‰è®ºæ–‡ PDF åˆ°æŒ‡å®šæ–‡ä»¶å¤¹
"""

import json
import os
import re
import time
import requests
from pathlib import Path
from urllib.parse import urlparse, parse_qs
from bs4 import BeautifulSoup

# é…ç½®
OUTPUT_DIR = "ssrn_papers"
FAILED_LOG_FILE = "failed_downloads.json"  # å¤±è´¥è®°å½•æ–‡ä»¶
DELAY_BETWEEN_REQUESTS = 1  # è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°

def extract_abstract_id(url):
    """ä» SSRN URL ä¸­æå– abstract_id"""
    match = re.search(r'abstract_id=(\d+)', url)
    if match:
        return match.group(1)
    return None

def get_download_url(abstract_id):
    """æ„é€  PDF ä¸‹è½½ URL"""
    # æ ¹æ®å›¾ç‰‡ä¸­çš„ HTML ç»“æ„ï¼Œä¸‹è½½é“¾æ¥æ ¼å¼ä¸ºï¼š
    # Delivery.cfm/{abstract_id}.pdf?abstractid={abstract_id}&mirid=1
    base_url = "https://papers.ssrn.com/sol3"
    download_url = f"{base_url}/Delivery.cfm/{abstract_id}.pdf?abstractid={abstract_id}&mirid=1"
    return download_url

def download_pdf(url, output_path, abstract_id):
    """ä¸‹è½½ PDF æ–‡ä»¶ï¼Œè¿”å› (æˆåŠŸæ ‡å¿—, é”™è¯¯ä¿¡æ¯)"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/pdf,application/octet-stream,*/*',
        'Referer': f'https://papers.ssrn.com/sol3/papers.cfm?abstract_id={abstract_id}'
    }
    
    last_error = None
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(url, headers=headers, stream=True, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ PDF æ–‡ä»¶
            content_type = response.headers.get('Content-Type', '')
            if 'pdf' not in content_type.lower() and not url.endswith('.pdf'):
                # å¦‚æœä¸æ˜¯ PDFï¼Œå°è¯•ä»é¡µé¢ä¸­æå–ä¸‹è½½é“¾æ¥
                print(f"  âš ï¸  ç›´æ¥é“¾æ¥ä¸æ˜¯ PDFï¼Œå°è¯•è§£æé¡µé¢...")
                return download_from_page(abstract_id, output_path)
            
            # ä¿å­˜æ–‡ä»¶
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(output_path)
            if file_size > 0:
                print(f"  âœ… ä¸‹è½½æˆåŠŸ ({file_size / 1024:.1f} KB)")
                return (True, None)
            else:
                error_msg = "æ–‡ä»¶å¤§å°ä¸º 0"
                print(f"  âŒ {error_msg}")
                return (False, error_msg)
                
        except requests.exceptions.RequestException as e:
            last_error = str(e)
            print(f"  âš ï¸  å°è¯• {attempt + 1}/{MAX_RETRIES} å¤±è´¥: {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(2)
            else:
                # æœ€åä¸€æ¬¡å°è¯•ï¼Œä»é¡µé¢è§£æ
                print(f"  ğŸ”„ å°è¯•ä»é¡µé¢è§£æä¸‹è½½é“¾æ¥...")
                result = download_from_page(abstract_id, output_path)
                if not result[0]:
                    return (False, f"ç›´æ¥ä¸‹è½½å¤±è´¥: {last_error}; é¡µé¢è§£æä¹Ÿå¤±è´¥: {result[1]}")
                return result
    
    return (False, f"æ‰€æœ‰é‡è¯•å‡å¤±è´¥: {last_error}")

def download_from_page(abstract_id, output_path):
    """ä» SSRN é¡µé¢è§£æå¹¶ä¸‹è½½ PDFï¼Œè¿”å› (æˆåŠŸæ ‡å¿—, é”™è¯¯ä¿¡æ¯)"""
    page_url = f"https://papers.ssrn.com/sol3/papers.cfm?abstract_id={abstract_id}"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    }
    
    try:
        response = requests.get(page_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥ - æ ¹æ®å›¾ç‰‡ä¸­çš„ HTML ç»“æ„
        # <a href="Delivery.cfm/4517697.pdf?abstractid=4517697&amp;mirid=1" class="button-link primary">
        download_link = None
        
        # æ–¹æ³•1: æŸ¥æ‰¾å¸¦æœ‰ data-abstract-id å±æ€§çš„é“¾æ¥
        link = soup.find('a', {'data-abstract-id': abstract_id})
        if link and link.get('href'):
            download_link = link['href']
        else:
            # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å« "Download This Paper" æ–‡æœ¬çš„é“¾æ¥
            link = soup.find('a', string=re.compile('Download This Paper', re.I))
            if link and link.get('href'):
                download_link = link['href']
            else:
                # æ–¹æ³•3: æŸ¥æ‰¾ class åŒ…å« "button-link primary" çš„é“¾æ¥
                link = soup.find('a', class_=re.compile('button-link.*primary'))
                if link and link.get('href'):
                    download_link = link['href']
        
        if download_link:
            # å¤„ç†ç›¸å¯¹ URL
            if download_link.startswith('/'):
                download_url = f"https://papers.ssrn.com{download_link}"
            elif download_link.startswith('Delivery.cfm'):
                download_url = f"https://papers.ssrn.com/sol3/{download_link}"
            else:
                download_url = download_link
            
            # ä¸‹è½½ PDF
            pdf_headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/pdf,application/octet-stream,*/*',
                'Referer': page_url
            }
            
            response = requests.get(download_url, headers=pdf_headers, stream=True, timeout=30)
            response.raise_for_status()
            
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(output_path)
            if file_size > 0:
                print(f"  âœ… ä»é¡µé¢ä¸‹è½½æˆåŠŸ ({file_size / 1024:.1f} KB)")
                return (True, None)
            else:
                error_msg = "ä»é¡µé¢ä¸‹è½½çš„æ–‡ä»¶å¤§å°ä¸º 0"
                print(f"  âŒ {error_msg}")
                return (False, error_msg)
        else:
            error_msg = "æ— æ³•åœ¨é¡µé¢ä¸­æ‰¾åˆ°ä¸‹è½½é“¾æ¥"
            print(f"  âŒ {error_msg}")
            return (False, error_msg)
            
    except requests.exceptions.RequestException as e:
        error_msg = f"é¡µé¢è¯·æ±‚å¤±è´¥: {str(e)}"
        print(f"  âŒ {error_msg}")
        return (False, error_msg)
    except Exception as e:
        error_msg = f"ä»é¡µé¢ä¸‹è½½å¤±è´¥: {str(e)}"
        print(f"  âŒ {error_msg}")
        return (False, error_msg)

def sanitize_filename(filename):
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
    illegal_chars = '<>:"/\\|?*'
    for char in illegal_chars:
        filename = filename.replace(char, '_')
    # é™åˆ¶æ–‡ä»¶åé•¿åº¦
    if len(filename) > 200:
        filename = filename[:200]
    return filename

def main():
    # è¯»å– JSON æ–‡ä»¶
    json_path = Path("ruotong.json")
    if not json_path.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {json_path}")
        return
    
    print(f"ğŸ“– è¯»å– {json_path}...")
    with open(json_path, 'r', encoding='utf-8') as f:
        urls = json.load(f)
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(urls)} ä¸ª URL")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(OUTPUT_DIR)
    output_dir.mkdir(exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}\n")
    
    # ç»Ÿè®¡ä¿¡æ¯
    success_count = 0
    fail_count = 0
    skip_count = 0
    failed_downloads = []  # è®°å½•å¤±è´¥çš„ä¸‹è½½
    
    # ä¸‹è½½æ¯ä¸ªè®ºæ–‡
    for i, url in enumerate(urls, 1):
        abstract_id = extract_abstract_id(url)
        if not abstract_id:
            error_msg = "æ— æ³•ä» URL æå– abstract_id"
            print(f"[{i}/{len(urls)}] âŒ {error_msg}: {url}")
            fail_count += 1
            failed_downloads.append({
                'url': url,
                'abstract_id': None,
                'error': error_msg,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            })
            continue
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        output_path = output_dir / f"{abstract_id}.pdf"
        if output_path.exists():
            print(f"[{i}/{len(urls)}] â­ï¸  è·³è¿‡ {abstract_id} (æ–‡ä»¶å·²å­˜åœ¨)")
            skip_count += 1
            continue
        
        print(f"[{i}/{len(urls)}] ğŸ“¥ ä¸‹è½½ {abstract_id}...")
        
        # å°è¯•ç›´æ¥ä¸‹è½½
        download_url = get_download_url(abstract_id)
        success, error_msg = download_pdf(download_url, output_path, abstract_id)
        
        if success:
            success_count += 1
        else:
            fail_count += 1
            # åˆ é™¤å¤±è´¥çš„æ–‡ä»¶
            if output_path.exists():
                output_path.unlink()
            # è®°å½•å¤±è´¥ä¿¡æ¯
            failed_downloads.append({
                'url': url,
                'abstract_id': abstract_id,
                'error': error_msg or "æœªçŸ¥é”™è¯¯",
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            })
        
        # å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        if i < len(urls):
            time.sleep(DELAY_BETWEEN_REQUESTS)
    
    # ä¿å­˜å¤±è´¥è®°å½•åˆ°æ–‡ä»¶
    if failed_downloads:
        log_path = Path(FAILED_LOG_FILE)
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(failed_downloads, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“ å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {log_path.absolute()}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*50)
    print("ğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
    print(f"  âœ… æˆåŠŸ: {success_count}")
    print(f"  â­ï¸  è·³è¿‡: {skip_count}")
    print(f"  âŒ å¤±è´¥: {fail_count}")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    if failed_downloads:
        print(f"  ğŸ“ å¤±è´¥è®°å½•: {FAILED_LOG_FILE}")
        print(f"\nå¤±è´¥çš„ URL åˆ—è¡¨:")
        for item in failed_downloads[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"    - {item['url']} ({item['error']})")
        if len(failed_downloads) > 10:
            print(f"    ... è¿˜æœ‰ {len(failed_downloads) - 10} ä¸ªå¤±è´¥è®°å½•ï¼Œè¯·æŸ¥çœ‹ {FAILED_LOG_FILE}")
    print("="*50)

if __name__ == "__main__":
    main()

